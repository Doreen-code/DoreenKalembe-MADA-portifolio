We shall start by loading a few packages that we think we might need for this data.

```{r, echo=FALSE, message=FALSE}

# Load packages
library(here)
library(knitr)
library(ggplot2)
library(readr)
library(readxl)
library(dplyr)
library(tidyverse)
library(haven)
library(Hmisc)
library(naniar)

```

We shall start by loading the data set and the exploring it to see what it contains. 
```{r}
# Read the CSV file
Mavo_data<- read_csv("data/Mavoglurant_A2121_nmpk.csv")

# Inspect the column names to find the correct one
colnames(Mavo_data)
```
lets go through the data set and see what it entails.
```{r}
glimpse(Mavo_data) 
```
The data set contains 2678 observations and 17 variables. 
code to make a plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose 
```{r}
Mavo_data%>%
  ggplot(aes(x=TIME, y= DV, group = ID,  color= factor(DOSE)))+
  geom_line()+ 
    labs(title = "DV over Time Stratified by Dose",
       x = "Time",
       y = "Outcome Variable (DV)",
       color = "Dose") +
  theme_minimal()

```
Keeping observations with OCC= 1
```{r}
Mavo_data2<- Mavo_data%>%
 filter(OCC!=2) # this will exclude everyone with occ=2.
 
```

lets exclude everyone with everyone with time =0.
and then create a another variable Y which is the sum of DV
```{r}
class(Mavo_data2)
```


```{r}
summary_data <- Mavo_data2 %>%
  filter(TIME != 0) %>%  # Remove rows where TIME = 0
   dplyr::group_by(ID) %>%
  dplyr::summarize(Y = sum(DV, na.rm = TRUE), .groups = "drop")  # Sum DV for each individual

print(summary_data)  
 
```
creating a data frame that contains observations with time =0.
```{r}
Mavo_data3<-Mavo_data2 %>%
  filter(TIME == 0)  # includes rows where TIME = 0
#This contains 120X17

```
using an appropiate join function to combine the two data frames

```{r}
joined_data<-full_join(summary_data, Mavo_data3, by ="ID")
```

Converting RACE and SEX to factor variables and keeping only these variables: Y,DOSE,AGE,SEX,RACE,WT,HT.
```{r}

# Converting  SEX to a factor 
joined_data$SEX <- as.factor(joined_data$SEX)

# Converting  race to a factor 
joined_data$RACE <- as.factor(joined_data$RACE)

# Check the structure of the joined dataset using the str function
str(joined_data)
```
finally, we need to select the variables that we shall need for our analaysis. 
```{r}
final_data<-joined_data%>%
  select(Y,DOSE,AGE,SEX,RACE,WT,HT)
```
 lets make some exploratory data analysis and some further cleaning 
 
```{r}
summary(final_data) #we use the summary function to see that summary statistics of each variable in the data set
```
 from the 120 observations,104 are males and 16 are females. thats if 1 is males and 2 is coded for females. According to the dataset we have, we are  not sure since the code book was not provided.
 
```{r}

# Compute summary statistics for all continuous variables
summary_table<- final_data %>%
  select(where(is.numeric)) %>%  # Select only numeric variables
  summarise(
    across(
      everything(), 
      list(
        Mean = ~mean(.x, na.rm = TRUE),
        Median = ~median(.x, na.rm = TRUE),
        SD = ~sd(.x, na.rm = TRUE),
        Min = ~min(.x, na.rm = TRUE),
        Max = ~max(.x, na.rm = TRUE)
      ),
      .names = "{.col}_{.fn}"  # Renaming format: VariableName_Statistic
    )
  ) %>%
  tidyr::pivot_longer(everything(), names_to = c("Variable", "Statistic"), names_sep = "_") %>%
  tidyr::pivot_wider(names_from = "Statistic", values_from = "value")

# Print summary
print(summary_table)

```
 
 lets check whether there is some correlation between variables
```{r}
# Select only numeric columns
numeric_data <- final_data %>% select(where(is.numeric))

# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Visualize correlation matrix
library(ggcorrplot)
ggcorrplot(cor_matrix, lab = TRUE)
```
From the above, looks like there is no variable that is highly correlated with the other.

lets explore our new variable Y using a histogram to see its distribution.
we shall also try to explore Y by the categorical variables
```{r}
# Histogram of Y variable (sum of DV)
ggplot(final_data, aes(x = Y)) + 
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) + 
  theme_minimal() + 
  labs(title = "Distribution of Y")

ggplot(final_data, aes(x = SEX, y = Y, fill = SEX)) + 
  geom_boxplot() + 
  theme_minimal() + 
  labs(title = "Y by SEX")

ggplot(final_data, aes(x = RACE, y = Y, fill = RACE)) + 
  geom_boxplot() + 
  theme_minimal() + 
  labs(title = "Y by RACE")
```
From the above, we see that Y is not normally distributed. it appears to be right skewed which means that some individuals have high sums of DV or simply we could have some potential outliers.

Also, assuming males are 1 and females are 2, then males have higher DV sums compared to females.

In regards to race, race 1 and 2 have almost similar DV sums compared to race 7 and 88.Though race 7 has higher sum compared to race 88.


lets see some scatter plot of Y as an outcome with the other continuous variables.
```{r}
final_data %>%
select(Y, DOSE, AGE, WT, HT) %>%
pairs()#this code will show the scatter plots of the continuous variables in the data set.
```
We shall now start working on  fitting the model using linear regression and logistic regression and the interprete the outputs.
Fit a linear model to the continuous outcome (Y) using the main predictor of interest, which we’ll assume here to be DOSE.
Fit a linear model to the continuous outcome (Y) using all predictors.
For both models, compute RMSE and R-squared and print them.
```{r}
#fitting a linear model with y as the outcome and dose as the predictor
model1<- lm(Y~DOSE,data=final_data)
summary(model1)

```
Lets try fitting Y with all the predictors.
```{r}
model2<-lm(Y~ DOSE+AGE+SEX+RACE+WT+HT, data = final_data)
summary(model2)

```
computing the RMSE and the R squared of the models
```{r}
r_squared_model1 <- summary(model1)$r.squared
r_squared_model2 <- summary(model2)$r.squared

#computing the RMSE
rmse_model1 <- sqrt(mean(model1$residuals^2))
rmse_model2 <- sqrt(mean(model2$residuals^2))

#Printing the rsmes and the r-squared
print(paste("Model 1 - RMSE:", rmse_model1, "R-squared:", r_squared_model1))
print(paste("Model 2 - RMSE:", rmse_model2, "R-squared:", r_squared_model2))
```
Fit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, which we’ll again assume here to be DOSE.
Fit a logistic model to SEX using all predictors.
For both models, compute accuracy and ROC-AUC and print them.

Let us try to fit another model with sex as the outcome

```{r}
# Fit logistic regression model with sex as the outcome and dose as the predictor.
logit_model1 <- glm(SEX ~ DOSE, data = final_data, family = binomial(link = "logit"))

# Fit logistic regression model with sex as the outcome and all the predictors.
logit_model2 <- glm(SEX ~ Y+ DOSE + AGE + RACE + WT + HT, data = final_data, family = binomial)

print(summary(logit_model1))
print(summary(logit_model2))
```
Let us try to calculate the odds ratios so as we can interprete these results better
```{r}
# Extract the model coefficients for model 1
coefficients <- summary(logit_model1)$coefficients[, 1]

# Calculate the odds ratios by exponentiating the coefficients
odds_ratios <- exp(coefficients)

# Print the odds ratios
print(odds_ratios)


# Extract the model coefficients for model 2
coefficients <- summary(logit_model2)$coefficients[, 1]

# Calculate the odds ratios by exponentiating the coefficients
odds_ratios_2<- exp(coefficients)

# Print the odds ratios
print(odds_ratios_2)

```

we shall then compute the accuracy and ROC-AUC and print them.
```{r}
# Model with only DOSE
predicted_probs1 <- predict(logit_model1, type = "response")  # Probabilities for SEX

# Model with all predictors
predicted_probs2 <- predict(logit_model2, type = "response")

# Convert probabilities to binary class (0 or 1)
predicted_class1 <- ifelse(predicted_probs1 > 0.5, 1, 0)
predicted_class2 <- ifelse(predicted_probs2 > 0.5, 1, 0)

# Accuracy calculation
accuracy1 <- mean(predicted_class1 == final_data$SEX)
accuracy2 <- mean(predicted_class2 == final_data$SEX)

print(paste("Logit mode 1 - Accuracy:", accuracy1))
print(paste("Logit Model 2 - Accuracy:", accuracy2))


```
Computing the ROC-AUC 
```{r}
library(pROC)             # this package will help us calculate the roc-auc


# Compute AUC for both models
auc1 <- roc(final_data$SEX, predicted_probs1)$auc
auc2 <- roc(final_data$SEX, predicted_probs2)$auc

print(paste("Logit Model 1 - AUC:", auc1))
print(paste("Logit Model 2 - AUC:", auc2))

```
We fitted the data to a linear model and a logistic regression model. When we fitted the linear regression model to the data, in the first model, we found that DOSE is a strong predictor of Y:

The coefficient (Estimate = 58.213) suggests that for every unit increase in DOSE, Y increases by approximately 58.21.
This effect is highly significant (p < 2e-16), meaning there is strong evidence that DOSE impacts Y.

Looking at the R-squared of this first linear model, we found that the simple linear model had an R-squared = 0.5156, meaning 51.56% of the variance in Y is explained by DOSE alone.


While DOSE is a significant predictor, the model might be too simple, as nearly half of the variation in Y is not explained which suggests that other variables might be important. As a result, we decided to fit the second linear model but this time around with other variables.
Second Model: Y ~ DOSE + AGE + SEX + RACE + WT + HT
DOSE remains a strong predictor:

The coefficient (Estimate = 59.935) is similar to the first model and still highly significant (p < 2e-16).

Additional variables:

WT (Weight) is also significant (p = 0.000471), with a negative coefficient (-23.047), suggesting that a unit increase in  weight decreases, Y.
SEX, AGE, and RACE do not contribute meaningful to prediction to Y in this dataset.
HT (Height) also does not appear to impact Y.

Model fit improved:

R-squared increased to 0.6193, meaning 61.93% of Y’s variance is explained by this model—better than the first model. In conclusion, DOSE remains the strongest predictor in both models, Adding more variables improves model fit, but only WT shows a significant effect in addition to DOSE.


Secondly,when we fitted a logistic regression with sex as the main outcome, first we fitted the model with only dose as a predictor

The coefficient for DOSE is -0.03175, meaning that as DOSE increases, the log-odds of the outcome (SEX) decrease slightly, but this effect is not statistically significant (p = 0.192).
The Intercept represents the baseline log-odds when DOSE = 0.

We fitted the second model with sex as the outcome this time round with other predictors. That is to say, SEX ~ Y + DOSE + AGE + RACE + WT + HT.
This is a multivariable logistic regression, including multiple predictors.
HT (Height) has a significant negative association with SEX (Estimate = -33.2, p = 0.0027). This suggests that an increase in height strongly decreases the log-odds of the outcome.

Other variables (Y, DOSE, AGE, RACE, WT) do not have significant p-values (> 0.05), meaning their effects are not strong enough to conclude they impact SEX.

Looking at the residual Deviance (32.077) is much lower than in the first model (92.431), suggesting the second model explains the data much better.
AIC (50.077)  in this second model is significantly lower than the first model’s AIC (96.431), meaning the second model fits better.

In conclusion, large AUC gap (0.592 vs. 0.980) confirms that adding more predictors greatly improves the model’s ability to differentiate between classes. The model with more predictors has better predictive power and is better.


```{r}
final_data2<-final_data%>%
  select( Y, DOSE, AGE, SEX, WT, HT)
colnames(final_data2)
```


```{r}
library(tidymodels)

#setting seed
set.seed(1234)
# Put 3/4 of the data into the training set 
data_split <- initial_split(final_data2, prop = 3/4)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)

# Check dimensions to ensure the data has been split successfully.
dim(train_data)
dim(test_data)
```
fitting the linear models using the train data.
```{r}
train_model<-lm(Y~DOSE, data = train_data)
train_model2<-lm(Y~DOSE+AGE + SEX +WT +  HT , data = train_data)
train_null <-lm(Y~ 1, data = train_data)

summary(train_model)
summary(train_model2)
```
lets evaluate performance and see how these two models work on the training data
```{r}


# Generate predictions on training data
#For null mode
pred_null<- predict(train_null, train_data)
# For model 1 (dose only)
pred_model1 <- predict(train_model, train_data)

# For model 2 (all predictors)
pred_model2 <- predict(train_model2, train_data)

# Let's Calculate RMSE for each model
# First, get the actual outcome values
actual_values <- train_data$Y 

# Calculate RMSE manually for null model 
rmse_null <- sqrt(mean((actual_values - pred_null)^2))

# Calculate RMSE manually for model 1
rmse_model1 <- sqrt(mean((actual_values - pred_model1)^2))

# Calculate RMSE manually for model 2
rmse_model2 <- sqrt(mean((actual_values - pred_model2)^2))

# Print results
cat("RMSE for null model :", rmse_null, "\n")
cat("RMSE for model with dose only:", rmse_model1, "\n")
cat("RMSE for model with all predictors:", rmse_model2, "\n")



```
According to the values of the RMSE, we see that the model with all predictors is performing well since its has the smallest value of the RMSE.

Lets try using the cv method
```{r}

library(rsample)

# Create 10-fold CV object
folds <- vfold_cv(train_data, v = 10)

# Define model specifications
lm_spec <- linear_reg() %>% 
  set_engine("lm")

# Model 1: Dose only model
model1_formula <- Y ~ DOSE # Replace 'outcome' with your actual outcome variable name

# Model 2: All predictors
model2_formula <- Y ~ .  # Replace 'outcome' with your actual outcome variable name

# Function to evaluate a model with CV
cv_evaluate <- function(formula, model_spec, folds_obj) {
  # Fit the model to each fold and evaluate
  cv_results <- folds_obj %>% 
    mutate(
      # Fit model on analysis set
      model = map(splits, ~model_spec %>% 
                    fit(formula, data = analysis(.x))),
      
      # Make predictions and calculate RMSE on assessment set
      pred = map2(model, splits, ~predict(.x, new_data = assessment(.y))),
      
      rmse = map2_dbl(pred, splits, ~rmse_vec(
        truth = assessment(.y)[[all.vars(formula)[1]]], 
        estimate = .x$.pred
      ))
    )
  
  # Return the results
  return(cv_results)
}

# Apply CV to both models
cv_results_model1 <- cv_evaluate(model1_formula, lm_spec, folds)
cv_results_model2 <- cv_evaluate(model2_formula, lm_spec, folds)

# Calculate mean RMSE across all folds
mean_rmse_model1 <- mean(cv_results_model1$rmse)
mean_rmse_model2 <- mean(cv_results_model2$rmse)

# Calculate standard errors
se_rmse_model1 <- sd(cv_results_model1$rmse) / sqrt(10)
se_rmse_model2 <- sd(cv_results_model2$rmse) / sqrt(10)

# Print results
cat("Model 1 (dose only) - CV Results:\n")
cat("  Mean RMSE:", mean_rmse_model1, "\n")
cat("  Standard Error:", se_rmse_model1, "\n\n")

cat("Model 2 (all predictors) - CV Results:\n")
cat("  Mean RMSE:", mean_rmse_model2, "\n")
cat("  Standard Error:", se_rmse_model2, "\n\n")

# Show individual fold results
cat("Model 1 - RMSE for each fold:\n")
print(cv_results_model1$rmse)

cat("\nModel 2 - RMSE for each fold:\n")
print(cv_results_model2$rmse)
```
Comparing the RMSE for CV With the previous RMS.

For model 1, the previous RMSE is 702.8078  while the cv RMSE is is 676.8427
For model 2, the previous RMSE is 627.441  while the cv RMSE is is 625.9641 

What changed and what did not change.


we see that there was a change in the RMSE for model 1 when we used the cross validation technique. The values of RSME dropped. However, there were no major changes in the the RSME in model two. The valued still reduced but by only 2 units.

What didn't change:

The null model RMSE remains the same regardless of the evaluation method since it always predicts the same value which is the mean.

Standard errors:

Model 2 has a lower standard error compared to model1 which contains only dose as a predictor. This shows that model 2 has a better variability in model performance across different subsets of data.

lets set another seed.
```{r}
# Set a different random number seed
set.seed(4567)  # Using a different seed than before

# Create new 10-fold CV object with the new seed
new_folds <- vfold_cv(train_data, v = 10)

# Apply CV to both models using the new folds
cv_results_model1_new <- cv_evaluate(model1_formula, lm_spec, new_folds)
cv_results_model2_new <- cv_evaluate(model2_formula, lm_spec, new_folds)

# Calculate mean RMSE across all folds with new seed
mean_rmse_model1_new <- mean(cv_results_model1_new$rmse)
mean_rmse_model2_new <- mean(cv_results_model2_new$rmse)

# Calculate standard errors with new seed
se_rmse_model1_new <- sd(cv_results_model1_new$rmse) / sqrt(10)
se_rmse_model2_new <- sd(cv_results_model2_new$rmse) / sqrt(10)

# Print comparison of results with different seeds
cat("Comparison of CV results with different random seeds:\n\n")

cat("Model 1 (dose only):\n")
cat("  - Mean RMSE with seed 1234:", mean_rmse_model1, "\n")
cat("  - Mean RMSE with seed 4567:", mean_rmse_model1_new, "\n")
cat("  - Difference:", mean_rmse_model1_new - mean_rmse_model1, "\n")
cat("  - Standard Error with seed 1234:", se_rmse_model1, "\n")
cat("  - Standard Error with seed 4567:", se_rmse_model1_new, "\n\n")

cat("Model 2 (all predictors):\n")
cat("  - Mean RMSE with seed 1234:", mean_rmse_model2, "\n")
cat("  - Mean RMSE with seed 4567:", mean_rmse_model2_new, "\n")
cat("  - Difference:", mean_rmse_model2_new - mean_rmse_model2, "\n")
cat("  - Standard Error with seed 1234:", se_rmse_model2, "\n")
cat("  - Standard Error with seed 4567:", se_rmse_model2_new, "\n\n")

# Check if the conclusion about which model is better remains the same
better_model_seed123 <- ifelse(mean_rmse_model1 < mean_rmse_model2, "Model 1", "Model 2")
better_model_seed456 <- ifelse(mean_rmse_model1_new < mean_rmse_model2_new, "Model 1", "Model 2")

cat("Better performing model:\n")
cat("  - With seed 1234:", better_model_seed123, "\n")
cat("  - With seed 4567:", better_model_seed456, "\n")


```
The overall performing model is still model 2 that has all the predictors. The changes we see are due to randomness maybe because of the seed change but generally, model 2 is still performing better than model 1.
